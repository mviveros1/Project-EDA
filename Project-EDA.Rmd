---
title: "Final Project-Maribel Viveros"
output:
  pdf_document: 
    latex_engine: xelatex
  word_document: default
  html_document: default
  
date: "2023-11-1"
---
# Exploratory Eye Gaze Analysis

## Introduction

  Eye-based activity recognition has become a focal point in Human-Computer Interaction (HCI) and Ubiquitous Computing (UbiComp). Primarily centered on identifying reading behaviors and associated cognitive processes, this field leverages distinct eye movement patterns that vary across different activities and factors. In a study conducted by Srivastava and colleagues in 2018, the researchers aimed to identify sedentary activities marked by minimal physical movements by analyzing eye movements. The broader goal of their study was concerned with designing computing systems that can proactively monitor daily activities, providing assistance or encouragement towards a healthier lifestyle. Srivastava et al. (2018) expanded on prior research, emphasizing the potential of eye tracking as a promising method for activity recognition. Additionally, the researchers developed a classifier combining existing low-level gaze features with novel mid-level gaze features. When applied to the dataset that included 24 participants engaged in a range of desktop computing activities, the outcomes demonstrated an overall accuracy of activity recognition.
  
  The original objective was to introduce a level of abstraction between low- and high-level gaze features, known as "mid-level gaze features." These mid-level gaze features don't necessitate an understanding of interface design but are based on intuitive knowledge about the types of eye movements associated with different activities. Intuitively, anyone can deduce the nature of a user's activity by examining gaze patterns. For instance, Srivastava and colleagues, cite a study which found reading patterns in Western languages commonly involve short saccades from left to right and long saccades from right to left at line breaks. Researchers leveraged this knowledge to identify fundamental components that typify specific activities within the time series data, Srivastava and colleagues’ hypothesis was that utilizing them as features could enhance classification results. 
  
  The data aimed to reveal the relationship and uncover the connection between eye movements and specific desktop-based activities. This understanding of human activities is valuable in several domains, including. The data contributes to the development of context-aware interactive systems. By understanding the context of users' activities, systems can provide more personalized and helpful support. This aligns with the researchers’ view that this finding has the potential to enhance users' daily tasks and encourage healthy lifestyles. The power relations and values influencing data inclusion and exclusion are the participant selection, the variety of eye behaviors, and the focus on reducing misclassification. The sensitivity to time windows also reflects the power researchers have in choosing data processing techniques. Ultimately, the study's objective is to simplify activity recognition by leveraging intuitive knowledge of eye movements and patterns.
  
  The study highlighted the ongoing challenge in the research community to achieve finer-grained activity recognition across a wide range of desktop-based activities using more precise eye-measures. Srivstava and colleagues hoped their findings would support the “design of computing systems that can proactively monitor daily activities and can either assist users with their daily tasks or encourage them to follow a healthy lifestyle.” In contrast to previous work centered on using eye movements for traditional human activity recognition, their study was oriented toward devising a methodology for distinguishing desktop activities. They constructed a classifier that combined 26 low-level gaze features from existing literature with 24 novel mid-level gaze features.


  Prior studies have shown that first fixation duration can serve as a measure of visual information acquisition (Holmqvist, et al., 2018). First fixation, or initial gaze, is the amount of time an individual spends fixating their gaze on a specific area of interest during an eye-tracking experiment. Given that prior researchers have already identified eye movements that are operationalized into mid-level, low-level, and high-level categories suitable for classification, my study seeks to ascertain which features are most appropriate for this purpose.   
  
  
  The proposed study explores the dataset from Srivastava et al. (2018) containing specific eye behaviors, including initial gaze, and time spent on activity, and examine if inital gaze and duration can be used for desktop activity recognition.  
  
  
# Setup

```{r}
suppressPackageStartupMessages({
  suppressWarnings(library(tidyverse))   # for working with the data
  suppressWarnings(library(skimr))       # generate a text-based overview of the data
  suppressWarnings(library(visdat))      # generate plots visualizing data types and missingness
  suppressWarnings(library(plotly))      # generate interactive plots
  suppressWarnings(library(readxl))
  library(downloader)
  library(openxlsx)
  library(viridis)
  library(tinytex)
  library(purrr)
  library(dplyr)
})
```

  This study holds significance as it offers empirical support for the advancement of eye-based activity recognition. By analyzing gaze data from 24 participants engaged in eight desktop activities, including time to first fixation (attention) and first fixation duration (initial object impression), this research will enrich our understanding of visual perception. Exploration of initial gaze differences, and examination of individual variations will identify features that contribute to the recognition of desktop activities. Ultimately, this research provides support for the development of advanced learning systems capable of personalized adaptation through the integration of eye-based activity recognition.The study has potential implications for education, technology, and cognitive science. The outcomes of this study can generate new insights into activity recognition using eye-tracking, benefiting both academia and the educational technology industry. 
I aim to delve into the intricacies of eye behavior, examining whether initial gaze holds clues regarding the induction of oculomotor behaviors in the context of activity recognition. Given that prior researchers have already identified eye movements that are operationalized into mid-level, low-level, and high-level categories suitable for classification, my study seeks to ascertain which features are most appropriate for this purpose.In summary, this research contributes to the enhancement of adaptive learning systems through the integration of eye-based activity recognition.

  The current research project posits: Can the application of first fixation metrics (a common eye-tracking measure) contribute to desktop activity recognition? This isn’t very precise, but that’s okay: Part of the goal of this EDA project is to clarify eye-metrics that contribute to understanding of visual perception.


## Methods

# Dataset
  The Srivastava and colleagues (2018) gaze dataset used in this project is publicly accessible on Kaggle. It encompasses raw gaze coordinates (x-y) and timestamp data obtained from 24 participants actively engaged in eight specific desktop activities; Read, Browse, Play, Search, Watch, Write, Debug, and Interpret. Each individual has to go through 3 sets of the task. 


```{r}
#The kaggle_url <- https://www.kaggle.com/datasets/namratasri01/
#eye-movement-data-set-for-desktop-activities/download?datasetVersionNumber=1" 
#Location of files for download
```

The initial gaze dataset was collected using the Tobii Pro X2-30 eye tracker.  The eye tracker was mounted on a 24-inch monitor. Participants were seated at an approximate distance of 60 centimeters from the display. Subsequent to the proficient execution of the manufacturer-prescribed 9-point calibration procedure, participants were afforded the liberty to move throughout the experimental session, capitalizing on the resilient tracking capabilities inherent to the infrared-illuminated eye tracker integrated within the screen. Depending on the specific task, participants were provided with directives to engage with the keyboard and/or mouse as dictated by the requirements, prefaced by a prompt to sustain attention on the assigned task.


The dataset is both complex and robust, consisting of 192 files, encompassing 1152 columns, and approximately 9,000 rows for each participant per activity. After visiting the kaggle website, the datafiles were downloaded into a 'data' folder. It is available on Kaggle at: https://www.kaggle.com/datasets/namratasri01/eye-movement-data-set-for-desktop-activities. The data set consists of eye-movements and has x and y and timestamp data.The variables 'participant', 'set', and 'activity' serve as identifiers.There were a total of 24 participants in the study, although they did not provide a reason for joining the study. 

```{r}
# Set the path to the data folder where you downloaded the files from Kaggle insert your file 
# path here' with the actual path to the downloaded files
file_paths <- file.path("data") 
```

To facilitate the examination of desktop activity identification, these individual participant datasets were merged into a unified file, resulting in a dataset with approximately 1505813 rows (observations); 6 columns (variables), 3 variables are handled as characters, and 3 as numeric.This is necessary to support the robustness of predictions derived from the integrated dataset.  Within the dataset, the variables x and y represent the spatial coordinates, while the timestamp indicates the specific time point when the gaze was at the corresponding x, y coordinate. The timestamp is instrumental in representing durations or the "time spent" on a particular activity. 

```{r}
# The combined resulting csv file  will be saved in your working directory.
output_file <- "combined_dataset.csv" 
csv_files <- list.files(file_paths, pattern = "\\.csv$", full.names = TRUE)
list_data <- map(csv_files, read.csv)
combined_data <- do.call(rbind, list_data)
write.csv(combined_data, file = output_file, row.names = FALSE)
```

As part of the EDA process, I looked at dimensions of the dataframe and column (variable) types outlined by Peng and Matsui (2016). First, I confirmed the dataset aligned with the metadata description, and three variables, x, y, and timestamp were listed as detailed in the combined dataset below.

```{r}
skim(combined_data) 
```

# Missing values
  Notably, the participant, set, and activity variables did not contain any missing values. Arguments in vis_miss() are useful for picking up patterns in missing values.   However, because of large data which caused a error to examine the data set for missing values 'sample_n' to draw a subset was used. 

```{r}
#sample to big so we'll draw a subset #but no missing data 
set.seed(123)
dataf_smol = sample_n(combined_data, 1000) 

vis_miss(dataf_smol)
```
  
# Data Summary

The metadata specifies there should be 24 participants, so I confirmed in the resulting combined data set, the 24 participants, 3 tasks, and 8 different activities. Additionally, the metadata also specifies how many activities the participants have to complete.  All the activities are included in the dataset. Thus, for the motivating question, the data set is appropriate as the eye gaze coordinates and timestamp is 100% complete. Also, the desktop activity data is 100% complete.

```{r}
count(combined_data, participant)
```

```{r}
count(combined_data, set)
```


```{r}
count(combined_data, activity)
```

# Validation

In the original methods section, the authors detail that all of the activities last about 5-6 minutes. Therefore the timestamps for the coordinates for each activity for all participants should not exceed this time, and indeed the max duration for one activity in going through all the values is ~6 minutes (399184 ms). A web search leads us to the website for the article where the data set is used: *https://www.researchgate.net/publication/329955224_Combining_Low_and_Mid-Level_Gaze_Features_for_Desktop_Activity_Recognition and published.*

```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    max_duration = max(timestamp - first(timestamp)),
    min_x = first(x),
    min_y = first(y)
  ) %>%
  arrange(desc(max_duration))
```


To verify that all the activities are within the expected time duration of the maximum limit of ~6 minutes (399184 ms) I identified the count of time stamps in the dataset less than 399184. A value  of FALSE in this context indicates that there are entries in the combined_data dataset where the timestamp is not greater, so all of the entries were completed in the expected time.

```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(timestamp) %>%
  mutate(too_long = timestamp > 399184) %>% 
  count(too_long)
```

## Results

The investigation into specific activities revealed discernible differences in both the average time spent on the first attended item and the location where participants directed their gaze. These differences, evident in the plotted data points, signify potential variations in how individual participants engage with different desktop activities. These nuanced observations provide valuable insights into cognitive processes and task engagement strategies, aligning with the overarching goal of enhancing users' daily tasks and promoting healthy lifestyles.

The preliminary results indicate that the application of first fixation metrics contributes to recognizing desktop activities. However, it is crucial to note the existence of individual disparities in eye behaviors. While the study sheds light on differences in first fixation times across various activities, it also highlights the need for further research to explore individual patterns and the generalizability of specific eye metrics.This study focuses on discerning patterns in visual engagement durations for distinct activities, aiming to identify variations in the amount of time allocated to each activity. The investigation addresses the question: Can we distinguish activities requiring prolonged or shorter periods of visual engagement?

The first fixation, represents the immediate processing of the attended stimulus and serves as a measure of attention. This data point will align to our question of which eye-metric can help us detect visual stimuli and activity? There should be specific first landing coordinates that differ across activity, and I would expect the duration of first time to fixation to differ across activity. 

 In the table below, mean durations were computed for each group (activity) by analyzing the time elapsed from the first timestamp to subsequent timestamps within each activity. The analysis utilized the differences between timestamps to derive the average duration of the initial instance for each activity. This was needed to examine the first fixation duration. 

  Longer mean durations, are indicative of heightened attention or engagement, and were observed for 'BROWSE' (151699.8 ms) and 'WATCH' (182171.3 ms) activities. Conversely, 'DEBUG' exhibited a shorter mean duration (142958.9 ms). Examination of the min x and min y coordinates, representing the initial landing positions for activities, aimed to ascertain where and if there are distinctions in the earliest gaze locations. The similar x, y coordinates, indicate differences of initial landing point based on activity type. 'READ' and 'WRITE' activity x and y coordinates have similar initial glances.


```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(timestamp) %>%
  group_by(activity) %>%
  summarise(mean_duration = mean(timestamp - first(timestamp)),
            min_x = first(x), 
            min_y = first(y))
```


# Plot 

The initial plot illustrates there is some variability in the location of the initial glances based on different activity. In review of the 'time spent' averages, the time participants spend observing a stimulus there is also a difference across activities. Additionally, there appears to be clear differences in average  fixation times by activity. 
    The plot shows the average duration (avg_duration),and minimum x-coordinate (min_x), and minimum y-coordinate (min_y) for each  'activity' and 'participant.' 
    
```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    avg_duration = mean(timestamp - first(timestamp)),
    min_x = first(x),
    min_y = first(y)
  ) %>%
  ggplot(aes(x = activity, y = avg_duration, fill = activity)) +
  geom_point() +
  labs(title = "Average Duration Time Across Activities for Each Participant",
       x = "Activity",
       y = "Average Duration (ms)",
       fill = "Activity") +
  theme_minimal()
```

The second graph focuses on assessing the average duration of the first fixation-first glance, which represents the time difference between the first and start of timestamps. The differences in the time spent during the first glance aims to reveal more noticeable distinctions in first fixation times across various activities and participants. The code calculates the first fixation duration by determining the time difference between the timestamp of the first fixation and the start time (0). 

```{r}
# Calculate average duration first fixation
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(participant, activity, timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    avg_duration = mean(timestamp - first(timestamp)),
    min_x = first(x),
    min_y = first(y),
    avg_first_fixation_duration = mean(c(0, diff(timestamp)))  
  ) %>%
  ggplot(aes(x = activity, y = avg_first_fixation_duration, fill = activity)) +
  geom_point() +
  labs(title = "Average Duration of First Fixation Across Activities for Each Participant",
       x = "Activity",
       y = "Average Duration of First Fixation (ms)",
       fill = "Activity") +
  theme_minimal()
```


Finally, in looking at patterns of eye-behaviors Its important to identify outliers. The plot below summarizes data related to participants' activities, then creates a boxplot, showing the distribution of the first x-coordinate across activities for each participant and the average duration of the first fixation. The plot is faceted by activity for comparison.This will give a better picture of if all the participants had particular regions of interest within a given activity. The box plot show the "avg_first_fixation_duration" variable for each "participant" categorized by "activity." Outliers, exceeding the whiskers, are depicted as individual points. The box plot and the scattered points, show the average first fixation durations are spread across various activities for each participant. The individual participant data points, represented as dots, are plotted using coordinates derived from "first_x" on the x-axis and "avg_first_fixation_duration" on the y-axis. The graph show differences in average time spent per activity, and highlights individual differences across the first landing position (x) and average time spent, as noted by the outliers.

```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(participant, activity, timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    avg_duration = mean(timestamp - first(timestamp)),
    first_x = first(x),  # Take the first x-coordinate
    avg_first_fixation_duration = mean(c(0, diff(timestamp))) 
  ) %>%
  ggplot(aes(x = avg_first_fixation_duration, y = first_x , fill = activity)) +
  geom_boxplot() +
  geom_point(position = position_jitter(height = 1), 
             size = .02, 
             alpha = 1) +  
  labs(title = "Distribution of First Fixation Duration across Activities for Each Participant",
       x = "Average First Fixation Duration (ms)",
       y = "first x coordinate",
       fill = "Activity") +
  theme_minimal()+
    facet_wrap(~ activity,scales = 'free_x')
  scale_x_continuous(labels = scales::number_format(scale = 2))
```

To translate the research question: Do all participants stare in relatively the same positions across the activities? The easy solution is to estimate location by plotting the x, or y coordinates since eye-movements are symmetrical, for each participant across the average first fixation time stamp for each activity.Only the row with the minimum timestamp (i.e., the first fixation) is provided below by each activity. 
  The preliminary results show, the first fixation across desktop activities is different but most participants look in the similar regions of the task area, which is dependent on the activity. 
  

```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(participant, activity, timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    avg_duration = mean(timestamp - first(timestamp)),
    first_x = first(x),  # Take the first x-coordinate
    avg_first_fixation_duration = mean(c(0, diff(timestamp)))
  ) %>%
  ggplot(aes(x = avg_first_fixation_duration, 
             y = first_x, 
             fill = activity)) +
  geom_point(position = position_jitter(height = 2), 
             size = 1, 
             alpha = 2) +
  labs(
    title = "Distribution of First X and Y Coordinates Across Activities for Each Participant",
    x = "First x coordinate", 
    y = "First y coordinate",
    fill = "Activity"  # Use fill instead of color for the legend title
  ) +
  theme_minimal() +
  facet_wrap(~ activity, scales = 'free_x')
```

The graph below further demonstrates the relationships between the first x coordinates and average of first fixation duration on the y-axis. It plots the participants average first fixation time on the x-axis and utilizes facets to categorize the data points based on different activities.  
There are distinct variations in the participants' duration gaze patterns across different activities and first time fixation across the stimulus area. 

```{r}
combined_data %>%
  filter(!is.na(activity)) %>%
  arrange(participant, activity, timestamp) %>%
  group_by(activity, participant) %>%
  summarise(
    avg_duration = mean(timestamp - first(timestamp)),
    first_x = first(x),  # Take the first x-coordinate
    min_y = first(y),
    avg_first_fixation_duration = mean(c(0, diff(timestamp)))  
  ) %>%
  ggplot(aes(x = first_x, y = avg_first_fixation_duration, color = activity)) +
  geom_line(size = 1) +  # Line graph for first x coordinates
  geom_point(position = position_jitter(height = 1), size = .02, alpha = 1) +  # Add individual points with jitter
  labs(
    title = "Distribution of First Fixation Duration and X-Coordinate Across Activities for Each Participant",
    x = "First x Coordinate" ,
    y = "Average First Fixation Duration (ms)",
    color = "Activity"
  ) +
  theme_minimal() +
  scale_x_continuous(labels = scales::number_format(scale = 2)) +
  facet_wrap(~ activity, scales = 'free_x')
```


## Limitations

The substantial size of the individual data sets, particularly with eye-metrics generating extensive data, presented anticipated challenges in the analysis phase. The determination of the "first time to fixation" instance to utilize proved intricate due to the richness of eye-tracking data. While eye-tracking studies conventionally rely on the averages of first time to fixation for analysis, I acknowledge the complexity of this decision. The variables influencing data inclusion and exclusion encompass participant selection, diverse eye behaviors, and a concerted effort to minimize misclassification. Additionally, the imposition of specific time window parameters reflects the researcher's discretion in choosing data processing techniques. Despite the complexity, the project yielded valuable insights, as observed differences in the location and time of the first fixation across activities aligned with the research question.

In shaping the trajectory of future research, a key consideration arises from the observed outliers in the box plot, indicating variability beyond the mean, upper, and lower quartiles for each activity. The emphasis on examining and understanding individual differences gains significance, urging the research community towards a more nuanced exploration of eye-metrics. Future studies should prioritize investigating these individual differences to enrich our understanding of visual engagement patterns and further refine desktop activity recognition. I anticipated challenges in the analysis. Determining which instance of "first time to fixation" to utilize proved difficult. Typically, eye tracking studies rely on the averages of first time to fixation for analysis so that is what I included. The complexity of the data aligned with my expectations and it was a challenging project. However, some notable differences in location of first fixation and time across activity were observed so the data fit the research question and I was able to make some conclusions.  

  In shaping the trajectory of future research, the focus should extend beyond the averages of first time to fixation.  The box plot revealed outliers beyond the mean, upper, and lower quartiles for each activity, indicating a need for focused investigation in individual differences.  Therefore, future studies should prioritize examining and understanding these individual differences, guiding the research community towards a more nuanced and impactful understanding of individual differences in eye-metrics. 

## Discussion

For each specific activity, both the average time spent on the first attended item and the location on the item where participants directed their gaze showed differences. The distinct differences in the plotted data points indicate potential variations in how individual participants engage with different desktop activities. These observations offer valuable insights into cognitive processes and task engagement strategies during the course of the study. By understanding the context of users' activities, systems can provide more personalized and helpful support. This aligns with the researchers’ view that these studies have the potential to enhance users' daily tasks and encourage healthy lifestyles. Ultimately, the study's objective is to simplify activity recognition by leveraging intuitive knowledge of eye movements and patterns.

Regarding the initial research question, *Can the application of first fixation metrics (a common eye-tracking measure) contribute to desktop activity recognition?* Initial observations of the first glance and first time of fixation reveal differences across various activities. However, the data also emphasize individual disparities in eye behaviors, suggesting a need for further research to explore individual patterns of eye behaviors and the extent to which certain eye metrics can be generalized.

## Implications and Future Directions

This research project contributes to the advancement of eye-based activity recognition, offering empirical support for the integration of eye metrics in understanding visual perception. The observed differences in first fixation times across activities suggest a potential avenue for enhancing activity recognition using eye-tracking data. Future studies should delve deeper into individual differences, focusing on outliers and variations beyond average metrics. The implications of this research extend to the development of context-aware interactive systems, personalized support for users' activities, and potential applications in education, technology, and cognitive science.

In conclusion, this study contributes to the foundation for future investigations into the intricate relationship between eye behaviors and desktop activities.


## Citations

Holmqvist, K., Nyström, M., Andersson, R., Dewhurst, R., Halszka, J., & van de Weijer, J. (2011). Eye Tracking : A Comprehensive Guide to Methods and Measures. Oxford University Press. http://ukcatalogue.oup.com/product/9780199697083.do

Srivastava, N., Newn, J., & Velloso, E. (2018). Combining Low and Mid-Level Gaze Features for Desktop Activity Recognition. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2(4), 189.

